{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "估算WSE的计算延迟。\n",
    "Core allocation：对于每个operator，假设其输入输出同时需要放在分配到的核上，把给定的wafer吃满，按照core数去分配。我们提前给一个合理的wafer setup。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "arch_config = {\n",
    "    'num_mac': 4,\n",
    "    'noc_bandwidth': 4,\n",
    "    'memory_size': 48 * 1024,\n",
    "    \"core_array_size\": 66 * 154,\n",
    "    \"reticle_array_size\": 12 * 8,\n",
    "}\n",
    "TOTAL_WSE_CORES = 66 * 154 * 12 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "\n",
    "onnx_model = onnx.load('gpt2.onnx')\n",
    "PRECISION = 2 # BF16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 估算模型参数量\n",
    "total_weight_size = 0\n",
    "for val in onnx_model.graph.input:\n",
    "    if \"onnx::\" in val.name:\n",
    "        continue\n",
    "    shape = [d.dim_value for d in val.type.tensor_type.shape.dim]\n",
    "    total_weight_size += reduce(lambda x, y: x * y, shape) * PRECISION\n",
    "print(f\"Total weight size: {total_weight_size / (1024 * 1024 * 1024)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 估计算子存储开销\n",
    "# 就认为是一个operator用到的所有input和output的大小之和\n",
    "# 如果intra-layer细粒度传输，每个算子的存储需求，就认为是output的需求\n",
    "tensor_name_2_shape = {}\n",
    "tensor_name_2_size = {}\n",
    "for val in chain(onnx_model.graph.input, onnx_model.graph.value_info, onnx_model.graph.output):\n",
    "    name = val.name\n",
    "    shape = [d.dim_value for d in val.type.tensor_type.shape.dim]\n",
    "    tensor_name_2_shape[name] = shape\n",
    "    tensor_name_2_size[name] = (reduce(lambda x, y: x * y, shape) if shape else 1) * PRECISION\n",
    "for val in onnx_model.graph.initializer:\n",
    "    name = val.name\n",
    "    shape = val.dims\n",
    "    tensor_name_2_shape[name] = shape\n",
    "    tensor_name_2_size[name] = (reduce(lambda x, y: x * y, shape) if shape else 1) * PRECISION\n",
    "    \n",
    "op_2_memory_consumption = {}\n",
    "for op_proto in onnx_model.graph.node:\n",
    "    name = op_proto.name\n",
    "    memory_consumption = 0\n",
    "    # for tensor in chain(op_proto.input, op_proto.output):\n",
    "    # only consider 1 copy of output\n",
    "    for tensor in op_proto.output:\n",
    "        memory_consumption += tensor_name_2_size[tensor]\n",
    "    op_2_memory_consumption[name] = memory_consumption \n",
    "\n",
    "total_memory = reduce(lambda x, y: x + y, list(op_2_memory_consumption.values()))\n",
    "print(f\"Total memory: {total_memory / (1024 * 1024 * 1024)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 估计传输开销\n",
    "# 只考虑input当中，位于value_info里面的，我们认为这些是需要inter-layer传输的\n",
    "# 还没除带宽，只考虑了传输量\n",
    "INTERMEDIATE_TENSOR_TYPE = 1\n",
    "WEIGHT_TENSOR_TYPE = 2\n",
    "tensor_name_2_type = {val.name: INTERMEDIATE_TENSOR_TYPE for val in onnx_model.graph.value_info}\n",
    "tensor_name_2_type.update({val.name: WEIGHT_TENSOR_TYPE for val in onnx_model.graph.input})\n",
    "tensor_name_2_type.update({val.name: WEIGHT_TENSOR_TYPE for val in onnx_model.graph.output})\n",
    "tensor_name_2_type.update({val.name: WEIGHT_TENSOR_TYPE for val in onnx_model.graph.initializer})\n",
    "\n",
    "op_2_comm_cost = {}\n",
    "for op_proto in onnx_model.graph.node:\n",
    "    name = op_proto.name\n",
    "    comm_cost = 0\n",
    "    for tensor in op_proto.input:\n",
    "        if tensor_name_2_type[tensor] == INTERMEDIATE_TENSOR_TYPE:\n",
    "            comm_cost += tensor_name_2_size[tensor]\n",
    "    op_2_comm_cost[name] = comm_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计所有张量的计算延迟\n",
    "# 只考虑总的计算量\n",
    "# 先看一下有啥类型的计算，然后把其中计算量大的统计出来即可\n",
    "def get_compute_cost(op_proto):\n",
    "    op_type = op_proto.op_type\n",
    "    if op_type == \"Conv\":\n",
    "        x, w = op_proto.input\n",
    "        x_shape, w_shape = tensor_name_2_shape[x], tensor_name_2_shape[w]\n",
    "        total_macs = reduce(lambda x, y: x * y, chain(x_shape, w_shape)) // x_shape[1]\n",
    "        y = op_proto.output\n",
    "        y_shape = tensor_name_2_shape[y]\n",
    "        total_macs += reduce(lambda x, y: x + y, y_shape)\n",
    "        return total_macs\n",
    "\n",
    "    elif op_type == \"Gemm\":\n",
    "        a, b, c = op_proto.input\n",
    "        a_shape, b_shape, c_shape = tensor_name_2_shape[a], tensor_name_2_shape[b], tensor_name_2_shape[c]\n",
    "        total_macs = a_shape[0] * (a_shape[1] + 1) * b_shape[1]\n",
    "        return total_macs\n",
    "        \n",
    "    elif op_type in [\"Add\", \"Sub\", \"Mul\", \"Div\"]:\n",
    "        a, b = op_proto.input\n",
    "        a_shape, b_shape = tensor_name_2_shape[a], tensor_name_2_shape[b]\n",
    "        is_scalar = lambda x: len(x) == 0\n",
    "        if is_scalar(a_shape) and is_scalar(b_shape):\n",
    "            return 1\n",
    "        elif is_scalar(a_shape) and not is_scalar(b_shape):\n",
    "            return tensor_name_2_size[b]\n",
    "        elif not is_scalar(a_shape) and is_scalar(b_shape):\n",
    "            return tensor_name_2_size[a]\n",
    "        else:\n",
    "            broadcast_shape = [max(i, j) for i, j in zip(a_shape, b_shape)]\n",
    "            return reduce(lambda x, y: x * y, broadcast_shape)\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "op_2_compute_cost = {}\n",
    "for op_proto in onnx_model.graph.node:\n",
    "    name = op_proto.name\n",
    "    op_2_compute_cost[name] = get_compute_cost(op_proto)\n",
    "# print(op_2_compute_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核的分配：\n",
    "# 先分配不卡计算的，按照memory需求给最少的核\n",
    "# 再分配卡计算的，按照计算需求吃满所有的核\n",
    "\n",
    "compute_bounded_op_type = [\n",
    "    'Gemm',\n",
    "]\n",
    "\n",
    "op_2_core_alloc = {\n",
    "    op.name: op_2_memory_consumption[op.name] // arch_config['memory_size'] for op in onnx_model.graph.node\n",
    "    if op.op_type not in compute_bounded_op_type\n",
    "}\n",
    "mem_bounded_total_core = reduce(lambda x, y: x + y, list(op_2_core_alloc.values()))\n",
    "compute_bounded_ops = [op for op in onnx_model.graph.node if op.op_type in compute_bounded_op_type]\n",
    "total_compute_bounded_cost = reduce(lambda x, y: x + y, \n",
    "                                    [op_2_compute_cost[op.name] for op in compute_bounded_ops])\n",
    "cur_total_core = TOTAL_WSE_CORES - mem_bounded_total_core\n",
    "assert cur_total_core > 0, cur_total_core\n",
    "op_2_core_alloc.update({\n",
    "    op.name: int((op_2_compute_cost[op.name] / total_compute_bounded_cost) * cur_total_core)\n",
    "    for op in compute_bounded_ops\n",
    "})\n",
    "print(op_2_core_alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核的分配\n",
    "# 就按照计算需求分配，不考虑爆内存\n",
    "\n",
    "# total_compute_cost = reduce(lambda x, y: x + y, list(op_2_compute_cost.values()))\n",
    "\n",
    "# op_2_core_alloc = {\n",
    "#     op.name: int(op_2_compute_cost[op.name] / total_compute_cost * TOTAL_WSE_CORES)\n",
    "#     for op in onnx_model.graph.node\n",
    "# }\n",
    "# print(op_2_core_alloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comm_latency(op):\n",
    "    core_alloc = op_2_core_alloc[op.name]\n",
    "    if core_alloc:\n",
    "        # return op_2_comm_cost[op.name] // (arch_config['noc_bandwidth'] * int(sqrt(op_2_core_alloc[op.name])))\n",
    "        return op_2_comm_cost[op.name] // (arch_config['noc_bandwidth'] * op_2_core_alloc[op.name])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_compute_latency(op):\n",
    "    core_alloc = op_2_core_alloc[op.name]\n",
    "    if core_alloc:\n",
    "        return op_2_compute_cost[op.name] // (arch_config['num_mac'] * op_2_core_alloc[op.name])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "op_2_comm_latency = {\n",
    "    op.name: get_comm_latency(op)\n",
    "    for op in onnx_model.graph.node\n",
    "}\n",
    "op_2_compute_latency = {\n",
    "    op.name: get_compute_latency(op)\n",
    "    for op in onnx_model.graph.node\n",
    "}\n",
    "\n",
    "print(max(list(op_2_comm_latency.values())))  # 计算开销\n",
    "print(max(list(op_2_compute_latency.values())))  # 传输开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('oneflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edc7a132f4c3b04ef5e6bc7fb6e902ce7520e532747fd14b82d99d5ee85e20f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
